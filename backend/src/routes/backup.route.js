import express from "express";
import fs from "fs";
import path from "path";
import { fileURLToPath } from "url";
import asyncHandler from "../lib/asyncHandler.js";
import { getPool } from "../config/db.pool.js";
import { requireAuth, requireRoles } from "../lib/auth.js";
import { validateBody, validateQuery } from "../lib/validate.js";
import { ok, created } from "../lib/respond.js";
import { recordAudit } from "../lib/audit.js";

const router = express.Router();

// Get __dirname equivalent for ES modules
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Path to database_backup folder (relative to project root)
const BACKUP_DIR = path.resolve(__dirname, "../../../..", "database_backup");

// Ensure backup directory exists
if (!fs.existsSync(BACKUP_DIR)) {
  fs.mkdirSync(BACKUP_DIR, { recursive: true });
}

// Tables to backup in order (respecting foreign key constraints)
const BACKUP_TABLES = [
  'Role',
  'LGU', 
  'SystemConfig',
  'User',
  'Driver',
  'Vehicle',
  'ViolationType',
  'Ticket',
  'TicketViolation',
  'Payment',
  'AuditLog',
  'BackupHistory'
];

// Helper function to escape SQL values
const escapeValue = (val) => {
  if (val === null || val === undefined) return 'NULL';
  if (typeof val === 'number') return val.toString();
  if (typeof val === 'boolean') return val ? '1' : '0';
  if (val instanceof Date) return `'${val.toISOString().slice(0, 19).replace('T', ' ')}'`;
  // Escape single quotes and backslashes
  return `'${String(val).replace(/\\/g, '\\\\').replace(/'/g, "\\'").replace(/\n/g, '\\n').replace(/\r/g, '\\r')}'`;
};

// Core backup function (reusable for manual and automatic backups)
async function performBackup(userId = null, isAutomatic = false, ipAddress = null) {
  const p = getPool();
  const triggerNote = isAutomatic ? "Automatic scheduled backup" : "Manual backup";
  
  // Record backup start - use null explicitly if userId is undefined
  const [backupRecord] = await p.execute(
    "INSERT INTO BackupHistory (action, status, triggered_by, notes) VALUES (?,?,?,?)",
    ["BACKUP", "IN_PROGRESS", userId !== undefined ? userId : null, triggerNote]
  );
  const backupId = backupRecord.insertId;
  
  try {
    let sqlDump = [];
    sqlDump.push("-- Unified Traffic Violation and Demerit System Database Backup");
    sqlDump.push(`-- Generated: ${new Date().toISOString()}`);
    sqlDump.push(`-- Type: ${isAutomatic ? 'Automatic' : 'Manual'}`);
    sqlDump.push(`-- Generated by User ID: ${userId || 'System'}`);
    sqlDump.push("");
    sqlDump.push("SET FOREIGN_KEY_CHECKS = 0;");
    sqlDump.push("");
    
    for (const tableName of BACKUP_TABLES) {
      try {
        // Get table data
        const [rows] = await p.execute(`SELECT * FROM \`${tableName}\``);
        
        if (rows.length > 0) {
          sqlDump.push(`-- Table: ${tableName}`);
          sqlDump.push(`-- Records: ${rows.length}`);
          
          // Get column names from first row
          const columns = Object.keys(rows[0]);
          const columnList = columns.map(c => `\`${c}\``).join(', ');
          
          sqlDump.push(`DELETE FROM \`${tableName}\`;`);
          
          // Generate INSERT statements
          for (const row of rows) {
            const values = columns.map(col => escapeValue(row[col])).join(', ');
            sqlDump.push(`INSERT INTO \`${tableName}\` (${columnList}) VALUES (${values});`);
          }
          sqlDump.push("");
        }
      } catch (tableErr) {
        sqlDump.push(`-- Error backing up table ${tableName}: ${tableErr.message}`);
        sqlDump.push("");
      }
    }
    
    sqlDump.push("SET FOREIGN_KEY_CHECKS = 1;");
    sqlDump.push("");
    sqlDump.push("-- End of backup");
    
    const sqlContent = sqlDump.join('\n');
    
    // Save to file
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19);
    const filename = `utvds_backup_${timestamp}${isAutomatic ? '_auto' : ''}.sql`;
    const filepath = path.join(BACKUP_DIR, filename);
    
    fs.writeFileSync(filepath, sqlContent, 'utf8');
    
    // Update backup record as successful
    await p.execute(
      "UPDATE BackupHistory SET status = 'SUCCESS', completed_at = NOW(), notes = ? WHERE backup_id = ?",
      [`${triggerNote} - Saved to: ${filename}`, backupId]
    );
    
    // Audit log
    if (userId) {
      await recordAudit({ 
        user_id: userId, 
        action: isAutomatic ? "AUTO_BACKUP" : "DATABASE_BACKUP", 
        details: `Backup ID: ${backupId}, File: ${filename}`, 
        ip_address: ipAddress || null, 
        affected_table_id: backupId, 
        affected_table: "BackupHistory" 
      });
    }
    
    return { success: true, backupId, filename, filepath, sqlContent };
    
  } catch (err) {
    // Update backup record as failed
    await p.execute(
      "UPDATE BackupHistory SET status = 'FAILED', completed_at = NOW(), notes = ? WHERE backup_id = ?",
      [`Error: ${err.message}`, backupId]
    );
    throw err;
  }
}

// Automatic backup scheduler
let autoBackupInterval = null;

async function startAutoBackup() {
  // Stop existing interval if any
  if (autoBackupInterval) {
    clearInterval(autoBackupInterval);
    autoBackupInterval = null;
  }
  
  try {
    const p = getPool();
    
    // Get auto backup settings from SystemConfig
    const [enabledRow] = await p.execute("SELECT `value` FROM SystemConfig WHERE `key` = 'auto_backup_enabled'");
    const [intervalRow] = await p.execute("SELECT `value` FROM SystemConfig WHERE `key` = 'auto_backup_interval_hours'");
    
    const enabled = enabledRow[0]?.value === 'true';
    const intervalHours = parseInt(intervalRow[0]?.value) || 24; // Default 24 hours
    
    if (enabled && intervalHours > 0) {
      const intervalMs = intervalHours * 60 * 60 * 1000;
      
      console.log(`[AutoBackup] Starting automatic backup every ${intervalHours} hours`);
      
      autoBackupInterval = setInterval(async () => {
        try {
          console.log(`[AutoBackup] Performing scheduled backup...`);
          const result = await performBackup(null, true, null);
          console.log(`[AutoBackup] Backup completed: ${result.filename}`);
        } catch (err) {
          console.error(`[AutoBackup] Backup failed:`, err.message);
        }
      }, intervalMs);
      
      return { enabled: true, intervalHours };
    } else {
      console.log(`[AutoBackup] Automatic backup is disabled`);
      return { enabled: false, intervalHours };
    }
  } catch (err) {
    console.error(`[AutoBackup] Failed to start:`, err.message);
    return { enabled: false, error: err.message };
  }
}

// Initialize auto backup on module load (with delay to ensure DB is ready)
setTimeout(() => {
  startAutoBackup().catch(console.error);
}, 5000);

// Generate and download backup
router.get(
  "/generate",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    const userId = req.user?.user_id || null;
    
    const result = await performBackup(userId, false, req.ip);
    
    // Send as downloadable file
    res.setHeader('Content-Type', 'application/sql');
    res.setHeader('Content-Disposition', `attachment; filename="${result.filename}"`);
    res.send(result.sqlContent);
  })
);

// Trigger automatic backup manually (for testing)
router.post(
  "/auto-trigger",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    const userId = req.user?.user_id || null;
    
    const result = await performBackup(userId, true, req.ip);
    
    ok(res, { 
      backup_id: result.backupId, 
      filename: result.filename,
      message: "Automatic backup triggered successfully"
    });
  })
);

// Get auto backup settings
router.get(
  "/auto-settings",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    const p = getPool();
    
    const [enabledRow] = await p.execute("SELECT `value` FROM SystemConfig WHERE `key` = 'auto_backup_enabled'");
    const [intervalRow] = await p.execute("SELECT `value` FROM SystemConfig WHERE `key` = 'auto_backup_interval_hours'");
    
    ok(res, {
      enabled: enabledRow[0]?.value === 'true',
      interval_hours: parseInt(intervalRow[0]?.value) || 24
    });
  })
);

// Update auto backup settings
router.put(
  "/auto-settings",
  requireAuth,
  requireRoles(["Super Admin"]),
  validateBody({
    enabled: { type: "boolean", required: true },
    interval_hours: { type: "int", required: true }
  }),
  asyncHandler(async (req, res) => {
    const p = getPool();
    const { enabled, interval_hours } = req.body;
    const userId = req.user?.user_id || null;
    
    // Validate interval
    if (interval_hours < 1 || interval_hours > 168) {
      return res.status(400).json({ error: { message: "Interval must be between 1 and 168 hours (1 week)" } });
    }
    
    // Update settings
    await p.execute(
      "INSERT INTO SystemConfig (`key`, `value`, description) VALUES (?,?,?) ON DUPLICATE KEY UPDATE `value`=VALUES(`value`)",
      ["auto_backup_enabled", String(enabled), "Enable automatic database backup"]
    );
    
    await p.execute(
      "INSERT INTO SystemConfig (`key`, `value`, description) VALUES (?,?,?) ON DUPLICATE KEY UPDATE `value`=VALUES(`value`)",
      ["auto_backup_interval_hours", String(interval_hours), "Automatic backup interval in hours"]
    );
    
    // Restart auto backup with new settings
    const autoResult = await startAutoBackup();
    
    // Audit log
    await recordAudit({ 
      user_id: userId, 
      action: "AUTO_BACKUP_SETTINGS_UPDATE", 
      details: `Enabled: ${enabled}, Interval: ${interval_hours} hours`, 
      ip_address: req.ip, 
      affected_table_id: null, 
      affected_table: "SystemConfig" 
    });
    
    ok(res, { 
      enabled, 
      interval_hours,
      scheduler_status: autoResult
    });
  })
);

// List saved backup files
router.get(
  "/files",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    try {
      const files = fs.readdirSync(BACKUP_DIR)
        .filter(f => f.endsWith('.sql'))
        .map(filename => {
          const filepath = path.join(BACKUP_DIR, filename);
          const stats = fs.statSync(filepath);
          return {
            filename,
            size_bytes: stats.size,
            created_at: stats.birthtime,
            modified_at: stats.mtime,
            is_automatic: filename.includes('_auto')
          };
        })
        .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
      
      ok(res, files);
    } catch (err) {
      ok(res, []);
    }
  })
);

// Download a specific backup file
router.get(
  "/files/:filename",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    const { filename } = req.params;
    
    // Sanitize filename to prevent path traversal
    const sanitized = path.basename(filename);
    if (!sanitized.endsWith('.sql')) {
      return res.status(400).json({ error: { message: "Invalid file type" } });
    }
    
    const filepath = path.join(BACKUP_DIR, sanitized);
    
    if (!fs.existsSync(filepath)) {
      return res.status(404).json({ error: { message: "Backup file not found" } });
    }
    
    res.setHeader('Content-Type', 'application/sql');
    res.setHeader('Content-Disposition', `attachment; filename="${sanitized}"`);
    res.sendFile(filepath);
  })
);

// Delete a backup file
router.delete(
  "/files/:filename",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    const { filename } = req.params;
    const userId = req.user?.user_id || null;
    
    // Sanitize filename
    const sanitized = path.basename(filename);
    if (!sanitized.endsWith('.sql')) {
      return res.status(400).json({ error: { message: "Invalid file type" } });
    }
    
    const filepath = path.join(BACKUP_DIR, sanitized);
    
    if (!fs.existsSync(filepath)) {
      return res.status(404).json({ error: { message: "Backup file not found" } });
    }
    
    fs.unlinkSync(filepath);
    
    // Audit log
    await recordAudit({ 
      user_id: userId, 
      action: "BACKUP_FILE_DELETE", 
      details: `Deleted backup file: ${sanitized}`, 
      ip_address: req.ip, 
      affected_table_id: null, 
      affected_table: "BackupHistory" 
    });
    
    ok(res, { message: "Backup file deleted", filename: sanitized });
  })
);

// Restore database from SQL dump
router.post(
  "/restore",
  requireAuth,
  requireRoles(["Super Admin"]),
  asyncHandler(async (req, res) => {
    const p = getPool();
    const { sqlContent } = req.body;
    const userId = req.user?.user_id || null;
    
    if (!sqlContent || typeof sqlContent !== 'string') {
      return res.status(400).json({ error: { message: "SQL content is required" } });
    }
    
    // Record restore start
    const [restoreRecord] = await p.execute(
      "INSERT INTO BackupHistory (action, status, triggered_by, notes) VALUES (?,?,?,?)",
      ["RESTORE", "IN_PROGRESS", userId, "Database restore initiated"]
    );
    const restoreId = restoreRecord.insertId;
    
    try {
      // Split SQL into statements
      const statements = sqlContent
        .split(';')
        .map(s => s.trim())
        .filter(s => s && !s.startsWith('--'));
      
      let successCount = 0;
      let errorCount = 0;
      const errors = [];
      
      // Execute each statement
      for (const stmt of statements) {
        if (!stmt) continue;
        try {
          await p.execute(stmt);
          successCount++;
        } catch (stmtErr) {
          errorCount++;
          if (errors.length < 10) {
            errors.push(stmtErr.message);
          }
        }
      }
      
      const status = errorCount === 0 ? 'SUCCESS' : (successCount > 0 ? 'SUCCESS' : 'FAILED');
      const notes = `Executed ${successCount} statements. ${errorCount > 0 ? `${errorCount} errors.` : ''}`;
      
      // Update restore record
      await p.execute(
        "UPDATE BackupHistory SET status = ?, completed_at = NOW(), notes = ? WHERE backup_id = ?",
        [status, notes, restoreId]
      );
      
      // Audit log
      await recordAudit({ 
        user_id: userId, 
        action: "DATABASE_RESTORE", 
        details: `Restore ID: ${restoreId}, ${notes}`, 
        ip_address: req.ip, 
        affected_table_id: restoreId, 
        affected_table: "BackupHistory" 
      });
      
      ok(res, { 
        restore_id: restoreId, 
        status, 
        statements_executed: successCount, 
        errors_count: errorCount,
        error_samples: errors
      });
      
    } catch (err) {
      // Update restore record as failed
      await p.execute(
        "UPDATE BackupHistory SET status = 'FAILED', completed_at = NOW(), notes = ? WHERE backup_id = ?",
        [`Error: ${err.message}`, restoreId]
      );
      throw err;
    }
  })
);

// Create a backup entry (legacy - for manual record keeping)
router.post(
  "/",
  requireAuth,
  requireRoles(["Super Admin"]),
  validateBody({
    status: { type: "enum", values: ["SUCCESS", "FAILED", "IN_PROGRESS"], optional: true },
    notes: { type: "string", optional: true },
  }),
  asyncHandler(async (req, res) => {
    const p = getPool();
    const { status = "IN_PROGRESS", notes = null } = req.body;
    const userId = req.user?.user_id || null;
    const [result] = await p.execute(
      "INSERT INTO BackupHistory (action, status, triggered_by, notes) VALUES (?,?,?,?)",
      ["BACKUP", status, userId, notes]
    );
    created(res, { backup_id: result.insertId });
  })
);

// List backup/restore history with filters
router.get(
  "/history",
  requireAuth,
  requireRoles(["Super Admin"]),
  validateQuery({
    action: { type: "enum", values: ["BACKUP", "RESTORE"], optional: true },
    status: { type: "enum", values: ["SUCCESS", "FAILED", "IN_PROGRESS"], optional: true },
    user_id: { type: "int", optional: true },
    start_date: { type: "date", optional: true },
    end_date: { type: "date", optional: true },
    page: { type: "int", optional: true },
    pageSize: { type: "int", optional: true },
  }),
  asyncHandler(async (req, res) => {
    const p = getPool();
    const {
      action,
      status,
      user_id,
      start_date,
      end_date,
      page = 1,
      pageSize = 10,
    } = req.query;

    const where = [];
    const params = [];
    if (action) {
      where.push("action = ?");
      params.push(action);
    }
    if (status) {
      where.push("status = ?");
      params.push(status);
    }
    if (user_id) {
      where.push("triggered_by = ?");
      params.push(Number(user_id));
    }
    if (start_date) {
      where.push("started_at >= ?");
      params.push(start_date);
    }
    if (end_date) {
      where.push("started_at <= ?");
      params.push(end_date);
    }

    const whereSql = where.length ? `WHERE ${where.join(" AND ")}` : "";

    // Count total with same filters
    const [countRows] = await p.execute(
      `SELECT COUNT(*) AS total FROM BackupHistory ${whereSql}`,
      params
    );
    const total = countRows?.[0]?.total || 0;
    const pages = Math.ceil(total / Number(pageSize)) || 1;

    // Page query
    const offset = (Number(page) - 1) * Number(pageSize);
    const [rows] = await p.execute(
      `SELECT b.backup_id, b.action, b.status, b.started_at, b.completed_at, b.notes, b.triggered_by, u.username
       FROM BackupHistory b
       LEFT JOIN User u ON u.user_id = b.triggered_by
       ${whereSql}
       ORDER BY b.started_at DESC
       LIMIT ${Math.floor(Number(pageSize))} OFFSET ${Math.floor(Number(offset))}`,
      params
    );

    ok(res, rows, { page: Number(page), pageSize: Number(pageSize), total, pages });
  })
);

export default router;
